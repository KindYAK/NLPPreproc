Модульная программа для предобработки данных от ИАЦ

**Концепция**

Программа представляет собой расширяемую за счёт модульности систему для предобработки текстовых данных.
Необходима реализация модулей предобработки, в том числе: токенизаторы, лемматизаторы, классификаторы POS/NER/Sentiment, dependency parserы, очистка, tfidf, w2v и др.

Программа позволит путём простой настройки аргументов проводить различные варианты предобработки для сравнительной оценки различных методов на качество распознавания

Список полей и структура хранения данных является прототипом - предложения и расширение имеющийся структуры необходимы и приветствуются.

**Как установить и запустить**

`pip install -r requirements.txt`

`python -m textblob.download_corpora` (займёт какое время - скачивание корпуса NLTK)

`python main.py fname=Sample.xlsx --sample_processor=args`

После этого будет создан pickled файл `Sample_processed([('sample_processor', None)]).pickled`

**Входные данные**

Пример входных данных - файл `Sample.xlsx`

**Выходные данные**

Как выглядят выходные данные можно увидеть в файле `main.py` в процедуре `read_xlsx`

**Аргументы вызова**

Обязательный аргумент - fname - название source файла

Далее может идти перечень аргументов, подключающих те или иные модули обработки.
Название аргумента должно совпадать с названием .py файла
После знака = можно передать произвольный аргумент для выполнения обработчика (без пробелов). Например: `--sample-processor=some_argument,or_what_not`

**Для разработчиков**

Пример модуля обработки - файл `sample_processor.py`

Пожалуйста, устанавливайте пакеты с помощью `pipenv install {packagename}`

**Список реализованных модулей**

Необходимо реализовать:
1) Нарезка на предложения
2) Очистка текста (стоп слова, лишние символы и пр.)
3) Токенизатор - **Done (Textblob/NLTK - --textblox_tokenize)**
4) Лемматизатор
5) NER
6) POS
7) Sentiment
8) Dependency Parsing
9) Векторизация
10) ???
11) PROFIT
